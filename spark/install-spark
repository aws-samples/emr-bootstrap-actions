#!/bin/bash
set -x 
#
# Simple script to perform third-party install of Spark on EMR clusters.
#
# Arguments (optional):
#   -c <s3_config_file>  - The install-spark config file which tells the script where to find version specific install scripts and binaries, defaults to AWS provided config
#   -v <version>      - Ability to select a specific Spark build version as avialable via the config file, see https://github.com/awslabs/emr-bootstrap-actions/blob/master/spark/README.md
#   -b <buildid>      - May be used by the versioned Spark install script in order to request a specific build
#   -g		      - Enables Ganglia metrics (deprecated - Ganglia metrics will now be configured as long as Ganglia app is installed)
#   -x                - Sets the default Spark config for dedictate Spark job utilization [1 executor per node, all vcore and memory, all core nodes]
#   -u <s3path>       - Add the jars in the given S3 path to spark classpath in the user-provided directory (ahead of all other dependencies) 
#   -a		      - (Use cautiously) Place spark-assembly-*.jar ahead of all system jars on spark classpath
#   -l		      - Set the log level of log4j.logger.org.apache.spark for the driver, defaults to INFO (OFF,ERROR,WARN,INFO,DEBUG,ALL), passed as env variable SparkDriverLogLevel to script
#   -h		      - Include EMR Hive jars in Spark classpath, this provides the enhancements from EMR Hive though may be at compatibility cost of some Spark features which expect Hive 0.14
#   -d <key>=<value>  - Set <key> to <value> in spark-defaults.conf (may be specified multiple times, prefixing each key-value pair with -d)
#
#
#  For AWS provided Spark builds and installs refer to https://github.com/awslabs/emr-bootstrap-actions/blob/master/spark/README.md
#   
#
#Config file is a tab-delimited file with these expected fields -
# field1 - EMR AMI version (may be a, a.b or a.b.c format or 'default', default will be used if exact match cannot be found)
# field2 - Spark build version (the REQUESTED_VERSION is matched against this field, if not specified it will look for the term 'default', default will be used if exact match cannot be found)
# field3 - (optional) shell program to use to execute the install script, defaults to python
# field4 - s3 location of a specific install script for this AMI and Spark version (is ran with environment variables S3SparkInstallPath, SparkBuild and Ec2Region)
# field5 - s3 path to files that will be used for Spark binaries, how this field is used is up to the install script, sets enviroment variable 'SparkS3InstallPath'
# field6 - (optional) s3 path to the script executed when "-x" argument is used, defaults to AWS provided version if empty
# field7 - (optional) s3 path to the script executed when "-g" argument is used to install ganlia, defaults to AWS provided version if empty
# field8 - (optional) s3 location for Scala binaries to install, how this field is used is up to the install script, set enviroment variable 'ScalaS3Location'
# field9 - (optional) s3 base path for supporting reference files, for example, where to find configure-spark.bash, this file would be expected to be found in this path, set environment variable 'SparkS3SupportingFilesPath'
#-----------------------------------------------------
#
if [ -e "/mnt/sparkinstalled" ]
then
	echo "Spark install already ran."
	exit 0
fi
#
# Gather basic info
HADOOP_VERSION=`grep /mnt/var/lib/info/job-flow.json  -e hadoopVersion | cut -d ':' -f2 | cut -d'"' -f2`
AMI_VERSION=$(grep /mnt/var/lib/info/job-flow-state.txt -e amiVersion | cut -d'"' -f2)

if [ "$AMI_VERSION" == "" ]
then	
	#this is older ami that does not support ami version info
	if [ "$HADOOP_VERSION" == "2.2.0" ]
	then
		AMI_VERSION="3.0"
	elif [ "$HADOOP_VERSION" == "1.0.3" ]
	then
		AMI_VERSION="2.3"
	else
		AMI_VERSION=""
	fi
fi

#Process AMI version
OIFS="$IFS"
IFS='.'
read -a AMI_VERSION_ARRAY <<< "${AMI_VERSION}"
IFS="$OIFS"

#determine the current region and place into REGION
EC2AZ=$(curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone)
REGION="`echo \"$EC2AZ\" | sed -e 's:\([0-9][0-9]*\)[a-z]*\$:\\1:'`"


REQUESTED_VERSION="(not specified)"
CONFIG_FILE="(not specified)"

#prepare the directory structure
TMPLOCATION="/mnt/install-spark-$RANDOM"

mkdir -p $TMPLOCATION

LOCAL_CONFIG_FILE="$TMPLOCATION/config.file"

SPARK_MAX=0
USER_PROVIDED_JARS=""

SPARK_ASSEMBLY_CP_FIRST=0
SPARK_BUILD=""

SPARK_DRIVER_LOG_LEVEL="INFO"

USE_EMR_HIVE_JARS=0

SPARK_DEFAULTS_CONF_KEY_VALUES=()

#process command line arguments
while getopts "c:v:b:xu:al:hd:" opt; do
  case $opt in
    c)
      CONFIG_FILE=$OPTARG
      ;;
    v)
      REQUESTED_VERSION=$OPTARG
      ;;
    b)
      SPARK_BUILD=$OPTARG
      ;;
    x)
      SPARK_MAX=1
      ;;
    u)
      USER_PROVIDED_JARS=$OPTARG
      ;;
    a)
      SPARK_ASSEMBLY_CP_FIRST=1
      ;;
    l)
      SPARK_DRIVER_LOG_LEVEL=$OPTARG
      ;;
    h)
      USE_EMR_HIVE_JARS=1
      ;;
    d)
      SPARK_DEFAULTS_CONF_KEY_VALUES+=($OPTARG)
      ;;
  esac
done

echo "This script installs the third-party software stack Spark on an EMR cluster."
echo "Requested Spark version: $REQUESTED_VERSION"
echo "Hadoop Verison: $HADOOP_VERSION"
echo "Config file: $CONFIG_FILE"

#=====================

#Load config file
if [ "$CONFIG_FILE" == "(not specified)" ]
then
	echo "Using AWS provided config file"
	if [ "$REGION" == "eu-central-1" ]
	then
		CONFIG_FILE=s3://eu-central-1.support.elasticmapreduce/spark/config.file
	else
		CONFIG_FILE=s3://support.elasticmapreduce/spark/config.file
	fi
fi

#Download the config locally 
CMDOUT=$(hadoop fs -get "$CONFIG_FILE" $LOCAL_CONFIG_FILE)
CMDSIG=$?

if [ "$CMDSIG" -eq 0 ] && [ -e "$LOCAL_CONFIG_FILE" ]
then
	echo "For install-spark using config file $CONFIG_FILE locally copied to $LOCAL_CONFIG_FILE"
else
	echo "Unable to download $CONFIG_FILE to local destination"
	exit 1
fi

#Config file is a tab-delimited file with these expected fields -
# field1 - EMR AMI version (may be a, a.b or a.b.c format or 'default', default will be used if exact match cannot be found)
# field2 - Spark build version (the REQUESTED_VERSION is matched against this field, if not specified it will look for the term 'default', default will be used if exact match cannot be found)
# field3 - (optional) shell program to use to execute the install script, defaults to python
# field4 - s3 location of a specific install script for this AMI and Spark version
# field5 - s3 path to files that will be used for Spark binaries, how this field is used is up to the install script, sets enviroment variable 'SparkS3InstallPath'
# field6 - (optional) s3 path to the script executed when "-x" argument is used, defaults to AWS provided version if empty
# field7 - (optional) s3 path to the script executed when "-g" argument is used to install ganlia, defaults to AWS provided version if empty
# field8 - (optional) s3 location for Scala binaries to install, how this field is used is up to the install script, set enviroment variable 'ScalaS3Location'
# field9 - (optional) s3 base path for supporting reference files, for example, the script configure-spark.bash, this file would be expected to be found in this path, set environment variable 'SparkS3SupportingFilesPath'

#------------------
# Process through config file options
# first, look for exact AMI and Spark version matches
if [ "$REQUESTED_VERSION" == "(not specified)" ]
then	
	REQUESTED_VERSION="default"
fi


CMDOUT=$(grep -P "^$AMI_VERSION\t$REQUESTED_VERSION\t" $LOCAL_CONFIG_FILE)
CMDSIG=$?

if [ "$CMDSIG" -ne 0 ]
then	
	#exact match didn't work, try ami version a.b
	CMDOUT=$(grep -P "^${AMI_VERSION_ARRAY[0]}.${AMI_VERSION_ARRAY[1]}\t$REQUESTED_VERSION\t" $LOCAL_CONFIG_FILE)
	CMDSIG=$?
fi

if [ "$CMDSIG" -ne 0 ]
then
        #that ami version didn't work either, try ami version a
        CMDOUT=$(grep -P "^${AMI_VERSION_ARRAY[0]}\t$REQUESTED_VERSION\t" $LOCAL_CONFIG_FILE)
        CMDSIG=$?
fi

if [ "$CMDSIG" -ne 0 ]
then
        #try ami versoin as default
        CMDOUT=$(grep -P "^default\t$REQUESTED_VERSION\t" $LOCAL_CONFIG_FILE)
        CMDSIG=$?
fi



#bail out if we can't find a config line
if [ "$CMDSIG" -ne 0 ]
then
	echo "Unable to match a AMI version and Spark version to an entry in the config file."
	exit 1
fi

#read the config line into an array
OIFS="$IFS"
IFS=$'\t'
read -a CONFIG_FIELDS_ARRAY <<< "$CMDOUT"
IFS="$OIFS"

#-----------------------

# download the install script
CMDOUT=$(hadoop fs -get "${CONFIG_FIELDS_ARRAY[3]}" $TMPLOCATION/install-spark-script)
CMDSIG=$?

if [ "$CMDSIG" -ne 0 ]
then
	#Unable to download install script
	echo "Unable to retrieve ${CONFIG_FIELDS_ARRAY[3]}"
	exit 1
fi

#---

# execute the install script

if [ "${CONFIG_FIELDS_ARRAY[2]}" == "" ]
then
	CONFIG_FIELDS_ARRAY[2] = "python"
fi

export SparkS3InstallPath="${CONFIG_FIELDS_ARRAY[4]}"
#optional value that may or may not be defined, up to install-spark-script to know how to use it
export SparkBuild="$SPARK_BUILD"
export Ec2Region="$REGION"
export ScalaS3Location="${CONFIG_FIELDS_ARRAY[7]}"
export SparkDriverLogLevel="$SPARK_DRIVER_LOG_LEVEL"

#SparkS3SupportingFilesPath
if [ "${CONFIG_FIELDS_ARRAY[8]}" == "" ]
then
       if [ "$REGION" == "eu-central-1" ]
       then
               CONFIG_FIELDS_ARRAY[8]=s3://eu-central-1.support.elasticmapreduce/spark
       else
               CONFIG_FIELDS_ARRAY[8]=s3://support.elasticmapreduce/spark
       fi
fi
export SparkS3SupportingFilesPath="${CONFIG_FIELDS_ARRAY[8]%/}"


CMDOUT=$(${CONFIG_FIELDS_ARRAY[2]} $TMPLOCATION/install-spark-script)
CMDSIG=$?

if [ "$CMDSIG" -ne 0 ]
then
        #Installation was not successful, abort
        echo "Spark install script ${CONFIG_FIELDS_ARRAY[3]} was not successful, aborting"
        exit 1
fi

echo "Spark install from ${CONFIG_FIELDS_ARRAY[3]} complete"

touch /mnt/sparkinstalled

#---------------
# Perform additional installs and configs due to arguments

# field6 - (optional) s3 path to the script executed when "-x" argument is used, defaults to AWS provided version if empty
# field7 - (optional) s3 path to the script executed when "-g" argument is used to install ganlia, defaults to AWS provided version if empty


if [ -e /usr/sbin/gmond ] # If Ganglia is installed
then
	#We need the ganglia script
	if [ "${CONFIG_FIELDS_ARRAY[6]}" == "" ]
	then
	        if [ "$REGION" == "eu-central-1" ]
       		then
               		CONFIG_FIELDS_ARRAY[6]="s3://eu-central-1.support.elasticmapreduce/spark/install-ganglia-metrics"
        	else
                	CONFIG_FIELDS_ARRAY[6]="s3://support.elasticmapreduce/spark/install-ganglia-metrics"
        	fi
	fi

	CMDOUT=$(hadoop fs -get "${CONFIG_FIELDS_ARRAY[6]}" $TMPLOCATION/install-ganglia-metrics)
	CMDSIG=$?

	if [ "$CMDSIG" -ne 0 ]
	then	
		echo "Unable to download ${CONFIG_FIELDS_ARRAY[6]}, unable to execute gangia installation, skipping"
	else
		echo "Installing ganglia metrics with ${CONFIG_FIELDS_ARRAY[6]}"
		bash $TMPLOCATION/install-ganglia-metrics
	fi
		
fi

if [ $SPARK_MAX -eq 1 ]
then
        #We need the script
        if [ "${CONFIG_FIELDS_ARRAY[5]}" == "" ]
        then
                if [ "$REGION" == "eu-central-1" ]
                then
                        CONFIG_FIELDS_ARRAY[5]="s3://eu-central-1.support.elasticmapreduce/spark/maximize-spark-default-config"
                else
                        CONFIG_FIELDS_ARRAY[5]="s3://support.elasticmapreduce/spark/maximize-spark-default-config"
                fi
        fi

        CMDOUT=$(hadoop fs -get "${CONFIG_FIELDS_ARRAY[5]}" $TMPLOCATION/maximize-spark-default-config)
        CMDSIG=$?

        if [ "$CMDSIG" -ne 0 ]
        then
                echo "Unable to download ${CONFIG_FIELDS_ARRAY[5]}, unable to execute Spark dedicated configuration, aborting"
		exit 1
        else
                echo "Installing Spark dedicated configuration with ${CONFIG_FIELDS_ARRAY[5]}"
                bash $TMPLOCATION/maximize-spark-default-config
        fi

fi

if [ $USE_EMR_HIVE_JARS -eq 1 ]
then
        echo "Inlcuding EMR Hive jars in spark classpath."
	mkdir /home/hadoop/spark/classpath/hive
	/bin/ls /home/hadoop/.versions/hive-*/lib/*.jar | xargs -n 1 -I %% cp %% /home/hadoop/spark/classpath/hive
	cp -f /home/hadoop/spark/conf/spark-env.sh /home/hadoop/spark/conf/spark-env.sh.prev
	perl -p -i -e "s|SPARK_CLASSPATH=\"/|SPARK_CLASSPATH=\"/home/hadoop/spark/classpath/hive/*:/|g" /home/hadoop/spark/conf/spark-env.sh
	#make sure commons-codec older versions are removed
	find /home/hadoop/spark/classpath/ -name "*commons-codec-*" | cut -d'/' -f7 | sort -r | tail -n +2 | xargs -n 1 -I {} find /home/hadoop/spark/classpath/ -name "{}" -delete
fi


if [ $SPARK_ASSEMBLY_CP_FIRST -eq 1 ]
then
	echo "Placing spark-assembly-*.jar to beginning of spark classpath."
	SPARK_ASSEMBLY=$(ls -l /home/hadoop/spark/lib/spark-assembly-*.jar |  perl -p -i -e 's|.*(spark-assembly.*\.jar)|$1|g')
	cp -f /home/hadoop/spark/conf/spark-env.sh /home/hadoop/spark/conf/spark-env.sh.prev
	perl -p -i -e "s|SPARK_CLASSPATH=\"/|SPARK_CLASSPATH=\"/home/hadoop/spark/lib/$SPARK_ASSEMBLY:/|g" /home/hadoop/spark/conf/spark-env.sh
fi

if [ "$USER_PROVIDED_JARS" != "" ]
then
	#$USER_PROVIDED_JARS should be HDFS path, copy contents to ~/spark/classpath/user-provided/
	echo "Adding $USER_PROIVDED_JARS to Spark classpath"
	mkdir -p ~/spark/classpath/user-provided
	hadoop fs -get $USER_PROVIDED_JARS/* ~/spark/classpath/user-provided/
	cp -f /home/hadoop/spark/conf/spark-env.sh /home/hadoop/spark/conf/spark-env.sh.prev
	perl -p -i -e 's|SPARK_CLASSPATH="/|SPARK_CLASSPATH="/home/hadoop/spark/classpath/user-provided/*:/|g' /home/hadoop/spark/conf/spark-env.sh
fi	

if [ ${#SPARK_DEFAULTS_CONF_KEY_VALUES[@]} -gt 0 ]
then
	hadoop fs -get "$SparkS3SupportingFilesPath/configure-spark.bash"
	bash configure-spark.bash "${SPARK_DEFAULTS_CONF_KEY_VALUES[@]}"
fi

exit 0
