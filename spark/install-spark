#!/bin/bash
set -x -e
#
# Simple script to perform third-party install of Spark on EMR clusters.
#
# Arguments (optional):
#   -v <version>        - Ability to select a specific Spark build version according to supported list
#   -g		            - For Spark builds 1.1.0.c and later, enables Ganglia metrics
#   -l	<logging level>	- Sets log4j.rootCategory. Spark comes with INFO which might be too verbose. If so, try WARN.
#                           Possible values: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN
#   -x                  - Sets the default Spark config for dedictate Spark job utilization [1 executor per node, all vcore and memory, all core nodes]
#
# Supported versions
#
# Hadoop 1.0.3, AMI 2.3.x and up (excluding AMI 3.x and later)
#   Spark 0.8.1
#
# Hadoop 2.2.0, AMI 3.0.x
#   Spark 1.0.0
#
# Hadoop 2.4.0 (AMI 3.1.x and 3.2.0 - 3.2.3, 3.3.0-3.3.1) 
#   Spark 1.0.2
#
#   Spark 1.1.0   
#
#   Spark 1.1.0.b (httpclient 4.2.5 for aws sdk s3 integration)
#   Spark 1.1.0.c (kinesis examples, spark-submit deploy mode default to cluster, sql hive depenency fix)
#   Spark 1.1.0.d (kinesis jars added to lib, added JavaKinesisWordCountASLYARN example using YARN)
#   Spark 1.1.0.e (kinesis example sources added to examples dir, includes SPARK-3595 for correct S3 output handling)
#   Spark 1.1.0.f (install script change to support EMR AMI versions 3.2.3, 3.1.4 and later)
#   Spark 1.1.0.g (disables multipart upload for Hadoop output formats as workaround, Enables Pyspark support)
#   Spark 1.1.0.h (same as "g", rebuilt with git repo)  [NOTE: Last version of 1.1.0 release ]
#
# Hadoop 2.4.0 (AMI 3.3.x) 
#   Spark 1.1.1.a (Initial version of Spark's 1.1.1 release with select changes for working on EMR)
#   Spark 1.1.1.b (Include SPARK-3595 for EMR S3 output without temporary directory)
#   Spark 1.1.1.c (Change to hadoop-provided profile on build, Fix hive-site.xml support with hive-default.xml)
#   Spark 1.1.1.d (Addition of JVM options for GC, Add Hbase and Kinesis client jars available to classpath)
#   Spark 1.1.1.e (SparkSQL support for EMR S3 output without temporary directory)
#
# Hadoop 2.4.0 (AMI 3.3.x)
#   Spark 1.2.0.a (Initial build of Spark's 1.2.0 release)
#
#
# Experimental versions available (designed to be ran with latest AMI available at time of build)
#   branch-1.1 ( "-v 1.1 -b <buildId>")
#     2014112801 (includes SPARK-2848)
#     2014121700
#   branch-1.2 ( "-v 1.2 -b <buildId>")
#     2014120500
#     2014121100 (includes SparkSQL S3 write fix and multipart upload disabled workaround)
#     2014121700
#       
#
#   
#
#-----------------------------------------------------
#
HADOOP_VERSION=`grep /mnt/var/lib/info/job-flow.json  -e hadoopVersion | cut -d ':' -f2 | cut -d'"' -f2`
AMI_VERSION=$(grep /mnt/var/lib/info/job-flow-state.txt -e amiVersion | cut -d'"' -f2)

#Process AMI version
OIFS="$IFS"
IFS='.'
read -a AMI_VERSION_ARRAY <<< "${AMI_VERSION}"
IFS="$OIFS"

SPARK_081=http://elasticmapreduce.s3.amazonaws.com/samples/spark/0.8.1/install-spark-shark.sh
#SPARK_100=http://elasticmapreduce.s3.amazonaws.com/samples/spark/1.0.0/install-spark-shark-yarn.rb
SPARK_100=http://support.elasticmapreduce.s3.amazonaws.com/spark/1.0.0/install-spark-shark-yarn.rb
SPARK_102=http://support.elasticmapreduce.s3.amazonaws.com/spark/ami-3.1.1/install-spark-1.0.2.py
SPARK_110=http://support.elasticmapreduce.s3.amazonaws.com/spark/1.1.0/install-spark-1.1.0.py
SPARK_110b=http://support.elasticmapreduce.s3.amazonaws.com/spark/1.1.0/install-spark-1.1.0.b.py
SPARK_110c=http://support.elasticmapreduce.s3.amazonaws.com/spark/1.1.0/install-spark-1.1.0.c.py
SPARK_110d=http://support.elasticmapreduce.s3.amazonaws.com/spark/1.1.0/install-spark-1.1.0.d.py
SPARK_110e=http://support.elasticmapreduce.s3.amazonaws.com/spark/1.1.0/install-spark-1.1.0.e.py
SPARK_110f=http://support.elasticmapreduce.s3.amazonaws.com/spark/1.1.0/install-spark-1.1.0.f.py
SPARK_110g=http://support.elasticmapreduce.s3.amazonaws.com/spark/1.1.0/install-spark-1.1.0.g.py
SPARK_110h=http://support.elasticmapreduce.s3.amazonaws.com/spark/1.1.0/install-spark-1.1.0.h.py
SPARK_110h_before323=http://support.elasticmapreduce.s3.amazonaws.com/spark/1.1.0/install-spark-1.1.0.h.before323.py

SPARK_111a=http://support.elasticmapreduce.s3.amazonaws.com/spark/1.1.1/install-spark-1.1.1.a.py
SPARK_111b=http://support.elasticmapreduce.s3.amazonaws.com/spark/1.1.1/install-spark-1.1.1.b.py
SPARK_111c=http://support.elasticmapreduce.s3.amazonaws.com/spark/1.1.1/install-spark-1.1.1.c.py
SPARK_111d=http://support.elasticmapreduce.s3.amazonaws.com/spark/1.1.1/install-spark-1.1.1.d.py
SPARK_111e=http://support.elasticmapreduce.s3.amazonaws.com/spark/1.1.1/install-spark-1.1.1.e.py
SPARK_111f=http://support.elasticmapreduce.s3.amazonaws.com/spark/1.1.1/install-spark-1.1.1.f.py

SPARK_120a=http://support.elasticmapreduce.s3.amazonaws.com/spark/1.2.0/install-spark-1.2.0.a.py

SPARK_11x=http://support.elasticmapreduce.s3.amazonaws.com/spark/1.1/install-spark-1.1.py
SPARK_build=2014112801

SPARK_12x=http://support.elasticmapreduce.s3.amazonaws.com/spark/1.2/install-spark-1.2.py

GANGLIASCRIPT=http://support.elasticmapreduce.s3.amazonaws.com/spark/install-ganglia-metrics
INSTALLGANGLIA=0

SPARK_MAX_SCRIPT=http://support.elasticmapreduce.s3.amazonaws.com/spark/maximize-spark-default-config
SPARK_MAX=0

SPARK_CONFIGURE_LOG4J_SCRIPT=http://s3-eu-west-1.amazonaws.com/emr.intags.com/bootstrap/configure-log4j-config

REQUESTED_VERSION="(not specified)"
while getopts "v:gb:xl:" opt; do
  case $opt in
    v)
      REQUESTED_VERSION=$OPTARG
      ;;
    g)
      INSTALLGANGLIA=1
      ;;
    b)
      SPARK_build=$OPTARG
      ;;
    l)
      SPARK_LOG4J_ROOT_LEVEL=$OPTARG
      ;;
    x)
      SPARK_MAX=1
      ;;
  esac
done

echo "This script installs the third-party software stack Spark on an EMR cluster."
echo "Requested Spark version: $REQUESTED_VERSION"
echo "Hadoop Verison: $HADOOP_VERSION"

# Determine avialable version based on Hadoop version
if [ "$HADOOP_VERSION" == "1.0.3" ]
then
	echo "Available Spark versions: 0.8.1"
	echo "Installing Spark version 0.8.1"
	REQUESTED_VERSION="0.8.1"

elif [ "$HADOOP_VERSION" == "2.2.0" ]
then
	echo "Available Spark versions: 1.0.0"
	echo "Installing Spark verison 1.0.0"
	REQUESTED_VERSION="1.0.0"

elif [ "$HADOOP_VERSION" == "2.4.0" ] 
then
	echo "Available Spark versions: 1.0.2, 1.1.0, 1.1.0.b, 1.1.0.c, 1.1.0.d, 1.1.0.e, 1.1.0.f, 1.1.0.g, 1.1.0.h, 1.1.1.a, 1.1.1.b, 1.1.1.c, 1.1.1.d, 1.1.1.e, 1.2.0.a"

else
	echo "Unknown hadoop version, unable to install"
	exit 1
fi

#== At this time only Hadoop 2.4.0 EMR supports multiple Spark versions...check for good selection
if [ "${AMI_VERSION_ARRAY[0]}" == "3" ] && [ "${AMI_VERSION_ARRAY[1]}" == "3" ] && [ "$REQUESTED_VERSION" == "(not specified)" ]
then
	echo "No Spark version specified, selecting Spark 1.2.0.a"
	REQUESTED_VERSION="1.2.0.a"

elif [ "$HADOOP_VERSION" == "2.4.0" ] && [ "$REQUESTED_VERSION" == "(not specified)" ] 
then
	echo "No Spark version specified, selecting Spark 1.1.0.h"
	REQUESTED_VERSION="1.1.0.h"
else
	echo "Undefined version"
fi

if [ "$HADOOP_VERSION" == "2.4.0" ] && [ "$REQUESTED_VERSION" != "1.0.2" ] && [ "$REQUESTED_VERSION" != "1.1.0" ] && [ "$REQUESTED_VERSION" != "1.1.0.b" ] && [ "$REQUESTED_VERSION" != "1.1.0.c" ] && [ "$REQUESTED_VERSION" != "1.1.0.d" ] && [ "$REQUESTED_VERSION" != "1.1.0.e" ] && [ "$REQUESTED_VERSION" != "1.1.0.f" ] && [ "$REQUESTED_VERSION" != "1.1.0.g" ] && [ "$REQUESTED_VERSION" != "1.1.0.h" ] && [ "$REQUESTED_VERSION" != "1.1.1.a" ] && [ "$REQUESTED_VERSION" != "1.1" ]  && [ "$REQUESTED_VERSION" != "1.1.1.b" ] && [ "$REQUESTED_VERSION" != "1.1.1.c" ] && [ "$REQUESTED_VERSION" != "1.1.1.d" ] && [ "$REQUESTED_VERSION" != "1.2" ] && [ "$REQUESTED_VERSION" != "1.1.1.e" ] && [ "$REQUESTED_VERSION" != "1.1.1.f" ] && [ "$REQUESTED_VERSION" != "1.2.0.a" ]
then
	echo "Unknown Spark version requested given available Spark versions for this Hadoop version"
	exit 1
fi


#=============
# Take action according to version
if [ "$REQUESTED_VERSION" == "0.8.1" ]
then
	wget -O install-spark-script $SPARK_081 
	bash install-spark-script
elif [ "$REQUESTED_VERSION" == "1.0.0" ]
then
	wget -O install-spark-script $SPARK_100
	set +e
	ruby install-spark-script
	set -e
elif [ "$REQUESTED_VERSION" == "1.0.2" ]
then
        wget -O install-spark-script $SPARK_102
	python install-spark-script BA
elif [ "$REQUESTED_VERSION" == "1.1.0" ]
then
        wget -O install-spark-script $SPARK_110
        python install-spark-script BA
elif [ "$REQUESTED_VERSION" == "1.1.0.b" ]
then
        wget -O install-spark-script $SPARK_110b
        python install-spark-script BA
elif [ "$REQUESTED_VERSION" == "1.1.0.c" ]
then
        wget -O install-spark-script $SPARK_110c
        python install-spark-script BA
elif [ "$REQUESTED_VERSION" == "1.1.0.d" ]
then
        wget -O install-spark-script $SPARK_110d
        python install-spark-script BA
elif [ "$REQUESTED_VERSION" == "1.1.0.e" ]
then
        wget -O install-spark-script $SPARK_110e
        python install-spark-script BA
elif [ "$REQUESTED_VERSION" == "1.1.0.f" ]
then
        wget -O install-spark-script $SPARK_110f
        python install-spark-script BA
elif [ "$REQUESTED_VERSION" == "1.1.0.g" ]
then
        wget -O install-spark-script $SPARK_110g
        python install-spark-script BA
elif [ "$REQUESTED_VERSION" == "1.1.0.h" ]
then
	if [ "${AMI_VERSION_ARRAY[0]}" == "3" ] && [ "${AMI_VERSION_ARRAY[1]}" == "2" ] && [ "${AMI_VERSION_ARRAY[2]}" -lt 3 ]
	then
		wget -O install-spark-script $SPARK_110h_before323
	elif [ "${AMI_VERSION_ARRAY[0]}" == "3" ] && [ "${AMI_VERSION_ARRAY[1]}" == "1" ] 
	then
		wget -O install-spark-script $SPARK_110h_before323
	else
        	wget -O install-spark-script $SPARK_110h
	fi
        python install-spark-script BA
elif [ "$REQUESTED_VERSION" == "1.1.1.a" ]
then
        wget -O install-spark-script $SPARK_111a
        python install-spark-script BA
elif [ "$REQUESTED_VERSION" == "1.1.1.b" ]
then
        wget -O install-spark-script $SPARK_111b
        python install-spark-script BA
elif [ "$REQUESTED_VERSION" == "1.1.1.c" ]
then
        wget -O install-spark-script $SPARK_111c
        python install-spark-script BA
elif [ "$REQUESTED_VERSION" == "1.1.1.d" ]
then
        wget -O install-spark-script $SPARK_111d
        python install-spark-script BA
elif [ "$REQUESTED_VERSION" == "1.1.1.e" ]
then
        wget -O install-spark-script $SPARK_111e
        python install-spark-script BA
elif [ "$REQUESTED_VERSION" == "1.1.1.f" ]
then
        wget -O install-spark-script $SPARK_111e
        python install-spark-script BA
elif [ "$REQUESTED_VERSION" == "1.1" ]
then
        wget -O install-spark-script $SPARK_11x
        python install-spark-script $SPARK_build
elif [ "$REQUESTED_VERSION" == "1.2" ]
then
        wget -O install-spark-script $SPARK_12x
        python install-spark-script $SPARK_build
elif [ "$REQUESTED_VERSION" == "1.2.0.a" ]
then
        wget -O install-spark-script $SPARK_120a
        python install-spark-script BA
else
	echo "Unknown requested Spark version, $REQUESTED_VERSION"
	exit 1
fi

if [ -n "$SPARK_LOG4J_ROOT_LEVEL" ]
then
        wget -O configure-log4j-config ${SPARK_CONFIGURE_LOG4J_SCRIPT}
        bash configure-log4j-config ${SPARK_LOG4J_ROOT_LEVEL}
fi

#===
echo "Spark install complete"

if [ $INSTALLGANGLIA -eq 1 ]
then
	wget -O install-ganglia-metrics $GANGLIASCRIPT
	bash install-ganglia-metrics
fi

if [ $SPARK_MAX -eq 1 ]
then
        wget -O maximize-spark-default-config $SPARK_MAX_SCRIPT
        bash maximize-spark-default-config
fi



exit 0
